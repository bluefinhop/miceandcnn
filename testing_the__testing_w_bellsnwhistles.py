# -*- coding: utf-8 -*-
"""testing_the _testing_w_bellsnwhistles.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/112GaUc3Lcfm1PwgbRHQasT7c3jSKNRN5
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import random
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.model_selection import train_test_split
# %matplotlib inline
matplotlib.rcParams['figure.facecolor'] = '#ffffff'

seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
random.seed(seed)

# This is needed for CUDA (GPU) if available
if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

url = 'https://raw.githubusercontent.com/bluefinhop/miceandcnn/main/DATA_dp_wc_bc.csv'
df1 = pd.read_csv(url)

i = 1
while i < 3:
  df1.drop(columns=df1.columns[0], axis=1,  inplace=True)
  i += 1

df3 = df1.loc[df1['Tissue'] == 'preparation']
df4 = df1.loc[df1['Tissue'] == 'biopsy']

one_hot_df3 = pd.get_dummies(df3['Annotation'])
one_hot_df4 = pd.get_dummies(df4['Annotation'])

arrdf3labels = one_hot_df3.iloc[:, :4].to_numpy()
arrdf4labels = one_hot_df4.iloc[:, :4].to_numpy()

arrdf3 = df3.iloc[:, :-2].values
arrdf4 = df4.iloc[:, :-2].values

dfTraining = pd.DataFrame({
    "pixels": arrdf3.tolist(),
    "label": arrdf3labels.tolist()
})
dfTesting = pd.DataFrame({
    "pixels": arrdf4.tolist(),
    "label": arrdf4labels.tolist()
})
# dfTesting=dfTraining #debugging, uncomment above later and comment this off

seq_len = arrdf3.shape[1]
num_samples = arrdf3.shape[0]
seq_len2 = arrdf4.shape[1]
num_samples2 = arrdf4.shape[0]

class TimeSeriesDataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        x = torch.tensor(self.df["pixels"][idx], dtype=torch.float32).unsqueeze(0)
        y = torch.tensor(np.argmax(self.df["label"][idx]), dtype=torch.long)
        return x, y

train_dataset = TimeSeriesDataset(dfTraining)
test_dataset = TimeSeriesDataset(dfTesting)
frac_train=0.85
train_dataset, val_dataset = random_split(train_dataset, [int(frac_train * len(train_dataset)), len(train_dataset) - int(frac_train * len(train_dataset))])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)

class CNN(nn.Module):
    def __init__(self, num_cnn_layers, dropout_prob=0.5):
        super(CNN, self).__init__()
        layers = []
        in_channels = 1
        for i in range(num_cnn_layers):
            out_channels = 10
            layers.append(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1))
            layers.append(nn.BatchNorm1d(out_channels))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_prob))
            if (i + 1) % 25 == 0:
                layers.append(nn.MaxPool1d(kernel_size=2))
            in_channels = out_channels
        self.cnn = nn.Sequential(*layers)
        self.fc1 = nn.Linear(in_features=out_channels * (seq_len // (2 ** (num_cnn_layers // 25))), out_features=4)

    def forward(self, x):
        x = self.cnn(x)
        x = x.view(x.size(0), -1)  # Flatten the output of the last convolutional layer
        x = self.fc1(x)
        return x
		
#Instantiate the CNN model
cnn = CNN(num_cnn_layers=4, dropout_prob=0.5)

#Define the loss function and the optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)

#Define a function for training the model
def train_model(model, dataloader, loss_fn, optimizer):
	model.train()
	running_loss = 0.0
	correct = 0
	total = 0
	for inputs, labels in dataloader:
		optimizer.zero_grad()

		outputs = model(inputs)
		loss = loss_fn(outputs, labels)
		loss.backward()
		optimizer.step()

		running_loss += loss.item() * inputs.size(0)
		_, predicted = torch.max(outputs, 1)
		total += labels.size(0)
		correct += (predicted == labels).sum().item()

	epoch_loss = running_loss / total
	accuracy = correct / total

	return epoch_loss, accuracy
	
#Define a function for validating the model
def validate_model(model, dataloader, loss_fn):
	model.eval()
	running_loss = 0.0
	correct = 0
	total = 0
	with torch.no_grad():
		for inputs, labels in dataloader:
			outputs = model(inputs)
			loss = loss_fn(outputs, labels)

			running_loss += loss.item() * inputs.size(0)
			_, predicted = torch.max(outputs, 1)
			total += labels.size(0)
			correct += (predicted == labels).sum().item()

	epoch_loss = running_loss / total
	accuracy = correct / total

	return epoch_loss, accuracy
	
#Train the model
num_epochs = 100

for epoch in range(num_epochs):
	train_loss, train_acc = train_model(cnn, train_loader, loss_fn, optimizer)
	val_loss, val_acc = validate_model(cnn, val_loader, loss_fn)
	print(f"Epoch {epoch + 1}/{num_epochs}:")
	print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc * 100:.2f}%")
	print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc * 100:.2f}%")
	print()
	
#Evaluate the CNN model on test data
cnn.eval()
correct = 0
total = 0

with torch.no_grad():
	for inputs, labels in test_loader:
		outputs = cnn(inputs)
		_, predicted = torch.max(outputs, 1)
		total += labels.size(0)
		correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f"Accuracy on test data: {accuracy * 100:.2f}%")